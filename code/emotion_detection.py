"""Module pour la d√©tection d'√©motions via Deep Learning (webcam/image)."""

from __future__ import annotations

import base64
import io
import tempfile
from pathlib import Path
from typing import Dict, Optional, Tuple

import cv2
import numpy as np
from PIL import Image

# Essayer d'importer deepface pour la d√©tection d'√©motion par deep learning
try:
    from deepface import DeepFace
    DEEPFACE_AVAILABLE = True
except ImportError:
    DEEPFACE_AVAILABLE = False
    print("‚ö†Ô∏è  DeepFace non disponible. Installation: pip install deepface")

# Mapping des √©motions DeepFace vers nos √©motions
EMOTION_MAPPING = {
    "happy": "heureux",
    "sad": "triste",
    "angry": "colere",
    "fear": "peur",
    "surprise": "surprise",
    "neutral": "neutre",
    "disgust": "ennuy√©",
    # Pour "stress√©" et "nostalgique", on utilise des combinaisons
}

# Mapping inverse pour les √©motions non directement support√©es
EMOTION_FALLBACK = {
    "stress√©": "angry",  # Col√®re peut indiquer du stress
    "nostalgique": "sad",  # Tristesse peut √™tre nostalgique
}


def _analyser_qualite_image(img: np.ndarray, face_bbox: Optional[Tuple[int, int, int, int]] = None) -> Dict[str, any]:
    """
    Analyse la qualit√© de l'image pour la d√©tection d'√©motion.
    
    Retourne un dictionnaire avec:
    - brightness: luminosit√© moyenne (0-255)
    - brightness_status: "trop_sombre", "ok", "trop_lumineux"
    - face_detected: bool
    - face_size_ratio: ratio de la taille du visage par rapport √† l'image
    - messages: liste de messages d'aide
    """
    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
    messages = []
    
    # Calculer la luminosit√© (convertie en float Python pour compat JSON)
    if face_bbox is not None:
        x, y, w, h = face_bbox
        face_roi = gray[y:y+h, x:x+w]
        brightness = float(np.mean(face_roi))
    else:
        brightness = float(np.mean(gray))
    
    # √âvaluer la luminosit√©
    if brightness < 60:
        brightness_status = "trop_sombre"
        messages.append("üí° La luminosit√© est trop faible. Placez-vous dans un endroit plus lumineux ou augmentez la luminosit√© de votre √©cran.")
    elif brightness > 200:
        brightness_status = "trop_lumineux"
        messages.append("‚òÄÔ∏è La luminosit√© est trop forte. R√©duisez l'√©clairage ou √©loignez-vous de la source de lumi√®re.")
    else:
        brightness_status = "ok"
    
    # D√©tecter le visage si pas d√©j√† fait
    face_detected = face_bbox is not None
    face_size_ratio: float = 0.0
    
    if face_bbox is not None:
        x, y, w, h = face_bbox
        img_area = img.shape[0] * img.shape[1]
        face_area = int(w) * int(h)
        face_size_ratio = float(face_area / img_area) if img_area > 0 else 0.0
        
        # V√©rifier si le visage est assez grand
        if face_size_ratio < 0.05:
            messages.append("üìè Votre visage est trop petit. Approchez-vous de la cam√©ra.")
        elif face_size_ratio > 0.5:
            messages.append("üìè Votre visage est trop proche. √âloignez-vous un peu de la cam√©ra.")
        else:
            # V√©rifier le centrage
            center_x = x + w // 2
            center_y = y + h // 2
            img_center_x = img.shape[1] // 2
            img_center_y = img.shape[0] // 2
            
            offset_x = abs(center_x - img_center_x) / img.shape[1]
            offset_y = abs(center_y - img_center_y) / img.shape[0]
            
            if offset_x > 0.3 or offset_y > 0.3:
                messages.append("üìç Centrez votre visage dans le cadre.")
    else:
        messages.append("üë§ Aucun visage d√©tect√©. Assurez-vous que votre visage est bien visible et centr√©.")
    
    return {
        "brightness": brightness,
        "brightness_status": brightness_status,
        "face_detected": face_detected,
        "face_size_ratio": face_size_ratio,
        "messages": messages
    }


def detecter_emotion_image(image_data: bytes) -> Dict[str, any]:
    """
    D√©tecte l'√©motion √† partir d'une image (webcam ou upload) avec deep learning.
    
    Retourne un dictionnaire avec:
    - emotion: √©motion d√©tect√©e (str) ou None
    - face_bbox: (x, y, w, h) du visage d√©tect√© ou None
    - quality: informations sur la qualit√© de l'image
    - confidence: confiance de la d√©tection (0-1)
    """
    try:
        # Convertir les bytes en image OpenCV
        nparr = np.frombuffer(image_data, np.uint8)
        img = cv2.imdecode(nparr, cv2.IMREAD_COLOR)
        
        if img is None:
            return {
                "emotion": None,
                "face_bbox": None,
                "quality": {
                    "brightness": 0,
                    "brightness_status": "erreur",
                    "face_detected": False,
                    "face_size_ratio": 0,
                    "messages": ["‚ùå Impossible de d√©coder l'image. V√©rifiez le format."]
                },
                "confidence": 0.0
            }
        
        # D√©tecter le visage avec OpenCV
        face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')
        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
        faces = face_cascade.detectMultiScale(gray, 1.1, 4, minSize=(50, 50))
        
        face_bbox = None
        if len(faces) > 0:
            # Prendre le plus grand visage
            face_bbox = max(faces, key=lambda f: f[2] * f[3])
            x, y, w, h = face_bbox
        
        # Analyser la qualit√© de l'image
        quality = _analyser_qualite_image(img, face_bbox)
        
        # Si pas de visage d√©tect√©, retourner avec les messages d'aide
        if face_bbox is None:
            return {
                "emotion": None,
                "face_bbox": None,
                "quality": quality,
                "confidence": 0.0
            }
        
        # Utiliser DeepFace pour la d√©tection d'√©motion
        emotion_detected = None
        confidence = 0.0
        
        if DEEPFACE_AVAILABLE:
            try:
                # Extraire la r√©gion du visage
                x, y, w, h = face_bbox
                face_roi = img[y:y+h, x:x+w]
                
                # Sauvegarder temporairement pour DeepFace
                with tempfile.NamedTemporaryFile(suffix='.jpg', delete=False) as tmp_file:
                    tmp_path = tmp_file.name
                    cv2.imwrite(tmp_path, face_roi)
                
                try:
                    # Analyser avec DeepFace
                    result = DeepFace.analyze(
                        img_path=tmp_path,
                        actions=['emotion'],
                        enforce_detection=False,
                        silent=True
                    )
                    
                    # Extraire l'√©motion dominante
                    if isinstance(result, list):
                        result = result[0]
                    
                        emotions = result.get('emotion', {})
                        if emotions:
                            # Trouver l'√©motion avec la plus haute confiance
                            emotion_key, score = max(emotions.items(), key=lambda x: x[1])
                            confidence = float(score) / 100.0  # np.float32 -> float
                            
                            # Mapper vers nos √©motions
                            emotion_detected = EMOTION_MAPPING.get(str(emotion_key).lower(), "neutre")
                        
                finally:
                    # Nettoyer le fichier temporaire
                    Path(tmp_path).unlink(missing_ok=True)
                    
            except Exception as e:
                print(f"‚ö†Ô∏è  Erreur DeepFace: {e}")
                # Fallback vers m√©thode simple si DeepFace √©choue
                emotion_detected = _detecter_emotion_simple(img, face_bbox)
                confidence = 0.5
        else:
            # Fallback vers m√©thode simple si DeepFace n'est pas disponible
            emotion_detected = _detecter_emotion_simple(img, face_bbox)
            confidence = 0.3
        
        return {
            "emotion": emotion_detected,
            "face_bbox": tuple(int(v) for v in face_bbox) if face_bbox is not None else None,
            "quality": quality,
            "confidence": float(confidence)
        }
        
    except Exception as e:
        print(f"‚ùå Erreur d√©tection √©motion: {e}")
        import traceback
        traceback.print_exc()
        return {
            "emotion": None,
            "face_bbox": None,
            "quality": {
                "brightness": 0,
                "brightness_status": "erreur",
                "face_detected": False,
                "face_size_ratio": 0,
                "messages": [f"‚ùå Erreur lors de la d√©tection: {str(e)}"]
            },
            "confidence": 0.0
        }


def _detecter_emotion_simple(img: np.ndarray, face_bbox: Tuple[int, int, int, int]) -> str:
    """
    M√©thode de fallback simple bas√©e sur la luminosit√© et les contours.
    """
    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
    x, y, w, h = face_bbox
    face_roi = gray[y:y+h, x:x+w]
    
    mean_brightness = np.mean(face_roi)
    
    # Heuristiques simples
    if mean_brightness < 80:
        return "triste"
    elif mean_brightness > 180:
        return "heureux"
    else:
        return "neutre"


def detecter_emotion_webcam() -> Optional[str]:
    """
    Capture une image depuis la webcam et d√©tecte l'√©motion.
    """
    try:
        cap = cv2.VideoCapture(0)
        if not cap.isOpened():
            return None

        ret, frame = cap.read()
        cap.release()

        if not ret:
            return None

        # Convertir en bytes
        _, buffer = cv2.imencode('.jpg', frame)
        image_bytes = buffer.tobytes()

        result = detecter_emotion_image(image_bytes)
        return result.get("emotion")

    except Exception as e:
        print(f"Erreur webcam: {e}")
        return None


def image_base64_to_bytes(base64_string: str) -> bytes:
    """Convertit une image base64 en bytes."""
    if base64_string.startswith("data:image"):
        base64_string = base64_string.split(",")[1]
    return base64.b64decode(base64_string)
